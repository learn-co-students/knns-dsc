# knns-dsc

## Instructions

* [ ] Download / draw in a piece of paper the [PDF](https://github.com/learn-co-students/knns-dsc/blob/master/knn_worksheet.pdf) material BEFORE the lecture
* [ ] Follow the instructions at the top of the PDF BEFORE the lecture
* [ ] Git clone this repository to be able to go through the [SLIDES](https://github.com/learn-co-students/knns-dsc/blob/master/knn_classification.slides.html)
* [ ] AFTER the lecture take this [QUIZ](https://forms.gle/kq9jUSgLf2DAD9eW6) to check your understanding of today's content

## Learning objectives
* [ ] Compare different types of distance metrics
* [ ] Have an intuitive understanding of why/when could a KNN classifier be useful
* [ ] Define "Lazy" learner
* [ ] Identify the steps to follow in training any sklearn learner
* [ ] Fit a KNN classifier
* [ ] Manually tune a hyper-parameter
* [ ] Understand the connection between higher/lower values of K and the likelihood of the model to underfit/overfit

## Resources
Nearest Neighbors ([user guide](http://scikit-learn.org/stable/modules/neighbors.html)), KNeighborsClassifier ([class documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html))

[Videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) from An Introduction to Statistical Learning

Classification Problems and K-Nearest Neighbors (Chapter 2)
Introduction to Classification (Chapter 4)
Logistic Regression and Maximum Likelihood (Chapter 4)
